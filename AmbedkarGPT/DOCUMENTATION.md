AmbedkarGPT — Professional Project Documentation

Version: 1.0
Date: 2025-11-20
Author: AmbedkarGPT Project Team

---

1. Executive Summary

AmbedkarGPT is a compact Retrieval-Augmented Generation (RAG) system and evaluation framework developed to demonstrate a complete prototype for question answering over a curated corpus of Dr. B.R. Ambedkar's speeches. The project includes a functional RAG pipeline (`main.py`) and a measurement suite (`evaluation.py`) that evaluates retrieval and answer quality across multiple chunking strategies.

This documentation is intended for reviewers, hiring managers, and engineers who need a concise technical overview, setup instructions, run procedures, and troubleshooting guidance.

---

2. Project Scope and Objectives

- Build a functional, local RAG prototype that:
  - Ingests a small corpus of 6 speech excerpts.
  - Creates embeddings using Hugging Face `all-MiniLM-L6-v2`.
  - Stores and queries vectors with ChromaDB.
  - Generates answers via a local LLM (Ollama + Mistral 7B).
- Implement an evaluation framework that measures retrieval and answer quality across three chunking strategies and produces comparative results.

---

3. Deliverables

- `main.py` — Command-line RAG prototype (Assignment 1)
- `evaluation.py` — Evaluation framework (Assignment 2)
- `corpus/` — Six speech text files (source documents)
- `test_dataset.json` — 25 curated test questions and ground truths
- `chroma_db/` — Local ChromaDB store (created at runtime)
- `test_results.json` and `results_analysis.md` (generated by the evaluation)
- `requirements.txt`, `README.md`, `DOCUMENTATION.md`

---

4. System Architecture (High Level)

- Document ingestion → Text splitting → Embeddings → Vector store (Chroma) → Retrieval → LLM generation
- The evaluation framework runs the retrieval + generation pipeline for each question and measures:
  - Retrieval metrics: Hit Rate, MRR, Precision@K
  - Answer quality metrics: ROUGE-L, BLEU, Cosine similarity
  - Comparative analysis for chunk sizes: Small (approx. 250 chars), Medium (approx. 500 chars), Large (approx. 900 chars)

---

5. Environment & Prerequisites

- Operating System: Windows recommended for this guide (PowerShell examples provided). Linux/macOS supported.
- Python: 3.8 or newer
- Virtual environment: `venv` or `conda` recommended
- External services:
  - Ollama (local LLM runtime) — optional for dry-run but required for full generation
  - Local internet access (for initial Hugging Face model downloads)

Required Python packages are listed in `requirements.txt`.

---

6. Installation & Setup (Step-by-step)

1. Clone or copy the project into a working folder.
2. Create and activate a virtual environment (PowerShell example):

```powershell
cd "D:\Imp\OneDrive\Desktop\Kalpit AI\AmbedkarGPT"
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

3. Install dependencies:

```powershell
pip install --upgrade pip
pip install -r requirements.txt
```

4. (Optional) Install Ollama and pull the Mistral model:

```powershell
# Install Ollama per https://ollama.ai
ollama pull mistral
ollama status
```

Note: If you cannot run Ollama, the project still lets you run initialization and embedding steps (see dry-run option in next section).

---

7. Running the Prototype: `main.py`

Purpose: Interactive Q&A over the corpus.

Basic usage:

```powershell
python main.py
```

Behavior:
- Loads all `.txt` files in `corpus/`.
- Splits text into chunks (configurable).
- Builds embeddings and stores them in ChromaDB (persisted to `chroma_db/`).
- Starts an interactive prompt where you can ask questions and receive generated answers plus source documents.

CLI options (if implemented):
- `--dry-run` or `--init-only`: Only perform ingestion, splitting, embeddings, and vector store creation — do not call the LLM.
- `--chunk-size` and `--overlap`: Override default chunking parameters.

---

8. Running the Evaluation: `evaluation.py`

Purpose: Evaluate system performance across the `test_dataset.json` and compare chunking strategies.

Basic usage:

```powershell
python evaluation.py
```

Outputs:
- `results/test_results.json` — Per-question and aggregated metrics for each chunking strategy
- `results/results_analysis.md` — A readable analysis report with conclusions and recommendations

Notes:
- The evaluation may be time-consuming if the LLM must generate answers for all 25 questions across 3 chunking strategies. Use `--dry-run` to run only retrieval and metric scaffolding without LLM calls.

---

9. Data & Test Dataset

- The `corpus/` directory contains six curated speech excerpts (files named `speech1.txt` through `speech6.txt`).
- `test_dataset.json` contains 25 test items with fields: `id`, `question`, `ground_truth`, `source_documents`, `question_type`, `answerable`.

---

10. Outputs & Where to Find Them

- Vector store: `chroma_db/` (created at runtime)
- Evaluation outputs: `results/test_results.json`, `results/results_analysis.md`
- Logs: Print output to console; optionally redirect to a file when running long evaluations.

---

11. Troubleshooting (Common Issues)

- Red diagnostics in VS Code for `.txt` files: This is editor analysis noise. A workspace setting excludes `corpus/` from Python analysis. Reload VS Code if the diagnostics persist.
- Ollama connection refused: Ensure Ollama daemon is running and `ollama status` returns OK.
- Hugging Face downloads failing: Check internet connection and disk space. Consider pre-downloading models or using a smaller model.
- ChromaDB errors: Remove the existing `chroma_db/` folder and retry to re-initialize.

---

12. Converting this documentation to Word or PDF

To produce a formal Word (`.docx`) or PDF from this Markdown file, use `pandoc` (recommended) or open the Markdown in a text editor that supports export to those formats.

Example (PowerShell):

```powershell
# Install pandoc if not present, then:
pandoc DOCUMENTATION.md -o AmbedkarGPT_Documentation.docx
pandoc DOCUMENTATION.md -o AmbedkarGPT_Documentation.pdf
```

If you want, I can produce a `.docx` or `.pdf` for you and place it in the repository (requires a conversion step; I can add the generated files if you confirm).

---

13. Recommendations & Next Steps

- Add a `--dry-run` / `--init-only` flag to both `main.py` and `evaluation.py` to enable automated CI-style checks without running the LLM.
- Add unit tests for ingestion, chunking, and retrieval components to validate changes quickly.
- Add a small sample script that runs 1–2 evaluation questions as a smoke test to make CI faster.
- Consider packaging the environment setup into a Dockerfile to improve reproducibility.

---

14. Contact

For assistance with running the system, converting documentation, or adding CI automation, reply here and I will implement the requested changes.

---

End of Documentation
